{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install libraries"
      ],
      "metadata": {
        "id": "JflSond7ArZ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9GpP0zRqSOQ",
        "outputId": "e6d41ab5-8107-4663-eae7-b67197654543",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.24.4 in /usr/local/lib/python3.11/dist-packages (1.24.4)\n",
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.4\n",
            "    Uninstalling numpy-1.24.4:\n",
            "      Successfully uninstalled numpy-1.24.4\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade numpy==1.24.4\n",
        "!pip install --upgrade --force-reinstall gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install underthesea scikit-learn transformers torch sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e_9hV6aCBRjz",
        "outputId": "5cc55d84-36cd-43d0-ce00-f4559bdceeaa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: underthesea in /usr/local/lib/python3.11/dist-packages (6.8.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from underthesea) (8.1.8)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.11/dist-packages (from underthesea) (0.9.11)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from underthesea) (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from underthesea) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from underthesea) (2.32.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.4.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from underthesea) (6.0.2)\n",
            "Requirement already satisfied: underthesea-core==1.0.4 in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.0.4)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import main libraries"
      ],
      "metadata": {
        "id": "XMFyjTWTAk6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec, KeyedVectors"
      ],
      "metadata": {
        "id": "1UfwbLx0qao3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import data"
      ],
      "metadata": {
        "id": "V8K2UdeL_5ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s2q8J-Azsx-",
        "outputId": "25cb305e-506a-4edd-f7e2-55c0264fb426"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_excel('/content/drive/MyDrive/DATA ANALYSIS/NLP/nlp.xlsx')"
      ],
      "metadata": {
        "id": "K2JgBJNF3C5-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing"
      ],
      "metadata": {
        "id": "vFmgtkje_7oM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from underthesea import word_tokenize, pos_tag\n",
        "\n",
        "# Vietnamese stopwords (expand this list if needed)\n",
        "vi_stopwords = set([\n",
        "    'và', 'là', 'có', 'cho', 'của', 'rằng', 'một', 'những', 'các', 'được',\n",
        "    'với', 'để', 'thì', 'lại', 'tôi', 'anh', 'em', 'này', 'đó', 'ở', 'ra', 'vào',\n",
        "    'toàn'\n",
        "])\n",
        "\n",
        "slang_dict = {\n",
        "    \"ko\": \"không\",\n",
        "    \"hok\": \"không\",\n",
        "    \"k\": \"không\",\n",
        "    \"j\": \"gì\",\n",
        "    \"thik\": \"thích\",\n",
        "    \"cx\": \"cũng\",\n",
        "    \"mik\": \"mình\",\n",
        "    \"mk\": \"mình\",\n",
        "    \"bt\": \"bình thường\",\n",
        "    \"dc\": \"được\",\n",
        "    \"vs\": \"với\",\n",
        "    \"đc\": \"được\",\n",
        "    \"r\": \"rồi\",\n",
        "    \"wa\": \"quá\",\n",
        "    \"đk\": \"được\",\n",
        "    \"ko bt\": \"không biết\",\n",
        "    \"vl\": \"bậy bạ\",\n",
        "}\n",
        "\n",
        "def normalize_slang(text, slang_dict):\n",
        "    words = text.split()\n",
        "    normalized = [slang_dict.get(word, word) for word in words]\n",
        "    return \" \".join(normalized)\n",
        "\n",
        "invalid_indices = []\n",
        "valid_corpus = []\n",
        "\n",
        "# Order logic of preprocessing steps: Cleaning ➝ Tokenizing ➝ Filtering\n",
        "\n",
        "for idx, review in enumerate(data['review']):\n",
        "    # 1. Clean text: remove digits, lowercase, remove punctuation\n",
        "    review = re.sub(r\"\\d+\", \"\", review)\n",
        "    review = review.lower()\n",
        "    review = review.translate(str.maketrans('', '', string.punctuation))\n",
        "    review = re.sub(r'\\s+', ' ', review).strip()\n",
        "\n",
        "    # Extra step: Domain knowledge (normalizing slang, idiom, abbreviations)\n",
        "    review = normalize_slang(review, slang_dict)\n",
        "\n",
        "    # 2. Tokenize\n",
        "    tokens = word_tokenize(review, format=\"text\").split()\n",
        "\n",
        "    # 3. POS tagging\n",
        "    tagged = pos_tag(\" \".join(tokens))  # list of (word, POS)\n",
        "\n",
        "    # 4. Keep Nouns, Adjectives, Verbs only\n",
        "    filtered = [word for word, pos in tagged if pos.startswith(('N', 'A', 'M', 'V', 'R'))]\n",
        "\n",
        "    # 5. Remove stopwords and single-letter tokens\n",
        "    final_tokens = [word for word in filtered if word not in vi_stopwords and len(word) > 1]\n",
        "\n",
        "    # If no valid tokens, log and save index\n",
        "    if len(final_tokens) == 0:\n",
        "        print(f\"Empty tokens after filtering at index {idx}:\")\n",
        "        print(f\"Original review: {data['review'][idx]}\")\n",
        "        invalid_indices.append(idx)\n",
        "    else:\n",
        "        valid_corpus.append(final_tokens)"
      ],
      "metadata": {
        "id": "HK13LS6rxLXn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a preprocessing function to reuse\n",
        "def preprocess_review(text):\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove digits\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "\n",
        "    # Normalize slang\n",
        "    text = normalize_slang(text, slang_dict)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text, format=\"text\").split()\n",
        "\n",
        "    # POS tagging\n",
        "    tagged = pos_tag(\" \".join(tokens))\n",
        "\n",
        "    # Filter for nouns, adjectives, and verbs\n",
        "    filtered = [word for word, pos in tagged if pos.startswith(('N', 'A', 'M', 'V', 'R'))]\n",
        "\n",
        "    # Remove stopwords and single-letter words\n",
        "    final_tokens = [word for word in filtered if word not in vi_stopwords and len(word) > 1]\n",
        "\n",
        "    return final_tokens"
      ],
      "metadata": {
        "id": "bhBU4Ys_PYMq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCvzPtNpuUiJ",
        "outputId": "aec577a7-94bd-4391-c533-37ec82a83ba0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ứng_dụng', 'luyện', 'đầy_đủ', 'cấp_độ'],\n",
              " ['quảng_cáo'],\n",
              " ['hiệu_quả', 'tuyệt_vời'],\n",
              " ['quảng_cáo', 'kinh_khủng'],\n",
              " ['ứng_dụng', 'đầy_đủ'],\n",
              " ['quảng_cáo', 'âm_thanh', 'tạp_âm'],\n",
              " ['quảng_cáo',\n",
              "  'quảng_cáo',\n",
              "  'chục',\n",
              "  'tua',\n",
              "  'một_chút',\n",
              "  'quảng_cáo',\n",
              "  'hát_hò',\n",
              "  'điên'],\n",
              " ['quảng_cáo', 'hoài_phiền'],\n",
              " ['ứng_dụng', 'hữu_ích', 'bổ_sung', 'memo', 'đoạn', 'văn'],\n",
              " ['hữu_ích_tra', 'nhanh_nghĩa', 'chính_xác', 'chữ', 'hán', 'tuyệt_vời'],\n",
              " ['rất', 'hay', 'ý_nghĩa', 'bé'],\n",
              " ['rất', 'không', 'quảng_cáo', 'rất', 'thoải_mái'],\n",
              " ['rất', 'tốt', 'bé', 'nhà', 'rất', 'thích'],\n",
              " ['rất', 'hay', 'bổ_ích', 'thú_vị'],\n",
              " ['tuyệt_vời', 'rất', 'vui_vẻ'],\n",
              " ['ứng_dụng', 'rất', 'hay', 'bổ_ích'],\n",
              " ['rất', 'hay', 'hiệu_quả', 'bé'],\n",
              " ['dino', 'đi', 'học', 'rất', 'tốt'],\n",
              " ['app',\n",
              "  'tệ',\n",
              "  'quá',\n",
              "  'lúc',\n",
              "  'không',\n",
              "  'gửi',\n",
              "  'mã_otp',\n",
              "  'lúc',\n",
              "  'kết_nối',\n",
              "  'thất_bại',\n",
              "  'lũ',\n",
              "  'ngu',\n",
              "  'làm',\n",
              "  'app'],\n",
              " ['chỗ',\n",
              "  'ghép',\n",
              "  'hàng',\n",
              "  'đủ',\n",
              "  'số',\n",
              "  'chữ',\n",
              "  'con',\n",
              "  'làm',\n",
              "  'đúng',\n",
              "  'rồi',\n",
              "  'bảo',\n",
              "  'sai'],\n",
              " ['trò', 'đã', 'bạn', 'nhiều', 'kiến_thức'],\n",
              " ['app', 'học', 'tốt', 'giúp', 'con', 'phát_triển', 'tư_duy'],\n",
              " ['ok'],\n",
              " ['app',\n",
              "  'tuyệt_vời',\n",
              "  'con',\n",
              "  'có_thể',\n",
              "  'vừa',\n",
              "  'học',\n",
              "  'vừa',\n",
              "  'chơi',\n",
              "  'nâng',\n",
              "  'cao',\n",
              "  'kiến_thức',\n",
              "  'khả_năng',\n",
              "  'tư_duy',\n",
              "  'nhạy_bén'],\n",
              " ['ứng_dụng',\n",
              "  'rất',\n",
              "  'tốt',\n",
              "  'việc',\n",
              "  'tư_duy',\n",
              "  'trí_tuệ',\n",
              "  'con',\n",
              "  'trẻ',\n",
              "  'bé',\n",
              "  'nhà',\n",
              "  'rất',\n",
              "  'hứng_thú',\n",
              "  'chương_trình',\n",
              "  'bkids'],\n",
              " ['chưa', 'con', 'thấy', 'app', 'rất', 'hữu_ích', 'cháu'],\n",
              " ['rất', 'phù_hợp', 'lứa', 'tuổi', 'cháu', 'nhà', 'người', 'nên', 'thử'],\n",
              " ['app',\n",
              "  'rất',\n",
              "  'hay',\n",
              "  'bổ_ích',\n",
              "  'con',\n",
              "  'tư_duy',\n",
              "  'học_hỏi',\n",
              "  'nhiều',\n",
              "  'cảm_ơn',\n",
              "  'bkids'],\n",
              " ['thi',\n",
              "  'online',\n",
              "  'kết_quả',\n",
              "  'đạt',\n",
              "  'không',\n",
              "  'đạt',\n",
              "  'không',\n",
              "  'công_bằng',\n",
              "  'làm',\n",
              "  'mất',\n",
              "  'thời_gian luyện_tập',\n",
              "  'câu',\n",
              "  'hỏi',\n",
              "  'không',\n",
              "  'phù_hợp',\n",
              "  'chương_trình học',\n",
              "  'đánh',\n",
              "  'lụi',\n",
              "  'hên',\n",
              "  'đúng',\n",
              "  'điểm',\n",
              "  'cao',\n",
              "  'hơn',\n",
              "  'điểm',\n",
              "  'bạn',\n",
              "  'khác',\n",
              "  'rất',\n",
              "  'nhiều',\n",
              "  'không',\n",
              "  'đạt',\n",
              "  'rà_soát',\n",
              "  'kiểu',\n",
              "  'điểm',\n",
              "  'thấp',\n",
              "  'đạt',\n",
              "  'điểm',\n",
              "  'cao',\n",
              "  'không'],\n",
              " ['thật_sự',\n",
              "  'không',\n",
              "  'phù_hợp',\n",
              "  'lứa',\n",
              "  'tuổi',\n",
              "  'tí',\n",
              "  'phần_mềm',\n",
              "  'ngáo_ngơ',\n",
              "  'dã_man',\n",
              "  'đứa',\n",
              "  'tuổi',\n",
              "  'người_lớn',\n",
              "  'làm',\n",
              "  'lần',\n",
              "  'mới',\n",
              "  'kịp_thời_gian',\n",
              "  'đồ',\n",
              "  'họa',\n",
              "  'nhận',\n",
              "  'chậm',\n",
              "  'vô_cùng'],\n",
              " ['không', 'học'],\n",
              " ['ứng_dụng',\n",
              "  'tuyệt_vời',\n",
              "  'bản_thân',\n",
              "  'vẫn',\n",
              "  'thích_cực',\n",
              "  'giao_diện',\n",
              "  'quá',\n",
              "  'đỉnh',\n",
              "  'nội_dung',\n",
              "  'khỏi',\n",
              "  'phải',\n",
              "  'bàn',\n",
              "  'quá',\n",
              "  'phù_hợp',\n",
              "  'giúp',\n",
              "  'bé',\n",
              "  'phát_triển',\n",
              "  'tư_duy'],\n",
              " ['sản_phẩm',\n",
              "  'rất',\n",
              "  'haygiúp',\n",
              "  'có_thể',\n",
              "  'chơi',\n",
              "  'dạy',\n",
              "  'con',\n",
              "  'mìnhrất_ý',\n",
              "  'nghĩamà',\n",
              "  'sản_phẩm',\n",
              "  'đội_ngũ',\n",
              "  'người',\n",
              "  'việtrất',\n",
              "  'bất_ngờ'],\n",
              " ['giao_diện', 'đẹp', 'nhiều', 'trò_chơi', 'bé', 'không', 'bị', 'chán'],\n",
              " ['rất', 'hài_lòng', 'ứng_dụng', 'rất', 'tốt', 'dễ', 'học'],\n",
              " ['tuyệt_vời'],\n",
              " ['cứ',\n",
              "  'báo',\n",
              "  'kết_nối mạng',\n",
              "  'bạn',\n",
              "  'không',\n",
              "  'ổn_định',\n",
              "  'khi',\n",
              "  'mạng',\n",
              "  'nhà',\n",
              "  'rất',\n",
              "  'mạnh cho_nên',\n",
              "  'bạn',\n",
              "  'đừng',\n",
              "  'tải'],\n",
              " ['ứng_dụng', 'rất'],\n",
              " ['học', 'tiếng', 'rất', 'hay'],\n",
              " ['trò_chơi', 'monkey', 'math', 'vui', 'tym', 'tum'],\n",
              " ['dữ_lăm_nha', 'người', 'tải'],\n",
              " ['học', 'chơi', 'chơi', 'học'],\n",
              " ['no', 'rất', 'hay', 'luôn'],\n",
              " ['kêu', 'vui_lòng', 'kiểm_tra', 'mạng', 'khi', 'mạng', 'nhà', 'rất', 'mạnh'],\n",
              " ['app',\n",
              "  'dạy',\n",
              "  'tiếng',\n",
              "  'cũng',\n",
              "  'bình_thường',\n",
              "  'giao_diện',\n",
              "  'đơn_giản',\n",
              "  'trò_chơi',\n",
              "  'cũng',\n",
              "  'đơn_giản',\n",
              "  'nói_chung',\n",
              "  'app',\n",
              "  'không',\n",
              "  'ho'],\n",
              " ['dùng', 'tốt', 'nghe', 'dễ', 'hiểu'],\n",
              " ['hơi', 'chán'],\n",
              " ['tốt'],\n",
              " ['mất', 'tiền', 'mới', 'học'],\n",
              " ['app', 'học', 'khá', 'ổn', 'cần', 'bổ_sung', 'thêm', 'phần', 'luyện', 'nói']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "from underthesea import sent_tokenize\n",
        "\n",
        "processed_words = []\n",
        "\n",
        "for text in valid_corpus:  # `corpus` contains preprocessed reviews (strings or token lists)\n",
        "    # Join list of tokens into text if needed\n",
        "    if isinstance(text, list):\n",
        "        text = ' '.join(text)\n",
        "\n",
        "    sentences = sent_tokenize(text)  # Vietnamese sentence segmentation\n",
        "    for sentence in sentences:\n",
        "        processed_words.append(simple_preprocess(sentence))  # Tokenize, lowercase, clean"
      ],
      "metadata": {
        "id": "dvtD5aO02Y6q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model selection"
      ],
      "metadata": {
        "id": "Kv87NwSgABqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word2Vec Model"
      ],
      "metadata": {
        "id": "MSe8Pxsa6X6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# 1. Train Word2Vec model\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=processed_words,  # list of tokenized sentences\n",
        "    vector_size=10,\n",
        "    window=5,\n",
        "    min_count=1, # might cause overfitting\n",
        "    workers=4,\n",
        "    sg=1,  # use skip-gram (better for smaller datasets), set sg=0 for CBOW\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# 2. View vocabularies\n",
        "print(\"Top 10 most frequent tokens:\", w2v_model.wv.index_to_key[:10])  # top 10 frequent tokens\n",
        "print(\"Vocabulary size:\", len(w2v_model.wv.index_to_key))\n",
        "\n",
        "# 3. Corpus size (number of sentences)\n",
        "print(\"Number of training sentences:\", w2v_model.corpus_count)\n",
        "\n",
        "# 4. Similar words to input (check if word exists in vocab first)\n",
        "word = 'điên'\n",
        "if word in w2v_model.wv:\n",
        "    print(\"Words most similar to 'điên':\", w2v_model.wv.most_similar(word))\n",
        "else:\n",
        "    print(f\"'{word}' not in vocabulary\")\n",
        "\n",
        "# 5. Vector shape for a word (again, check if it exists)\n",
        "word = 'quảng_cáo'\n",
        "if word in w2v_model.wv:\n",
        "    print(f\"Vector shape of '{word}':\", w2v_model.wv[word].shape)\n",
        "else:\n",
        "    print(f\"'{word}' not in vocabulary\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJbUcv4Q3Fjc",
        "outputId": "0358f021-0ddd-4124-b633-f751802d7a8d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most frequent tokens: ['rất', 'không', 'học', 'app', 'quảng_cáo', 'ứng_dụng', 'tốt', 'hay', 'con', 'bé']\n",
            "Vocabulary size: 189\n",
            "Number of training sentences: 50\n",
            "Words most similar to 'điên': [('mìnhrất_ý', 0.7684803009033203), ('nhiều', 0.728179931640625), ('âm_thanh', 0.7148966193199158), ('vẫn', 0.7006509900093079), ('đạt', 0.6749110221862793), ('vừa', 0.6685372591018677), ('dạy', 0.6450703740119934), ('phải', 0.5831232070922852), ('tư_duy', 0.5791965126991272), ('nghĩamà', 0.5183154940605164)]\n",
            "Vector shape of 'quảng_cáo': (10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def avg_word2vec(doc, model):\n",
        "    # filter out-of-vocab words\n",
        "    valid_words = [word for word in doc if word in model.wv.index_to_key]\n",
        "\n",
        "    # debug print\n",
        "    print(valid_words)\n",
        "\n",
        "    if not valid_words:\n",
        "        # fallback: return zero vector with same shape as model vector size\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "    # average vectors of all valid words\n",
        "    return np.mean([model.wv[word] for word in valid_words], axis=0)"
      ],
      "metadata": {
        "id": "96v-rYN55RSQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "cu_3vS9R83Rp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "929fc194-8220-4b10-e3ae-239c70a23fd4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Rg2vzhcU86Zh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# independent\n",
        "X_w2v = []\n",
        "# for i in tqdm(range(len(processed_words))):\n",
        "for i in range(len(processed_words)):\n",
        "  X_w2v.append(avg_word2vec(processed_words[i],w2v_model))\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_w2v = scaler.fit_transform(X_w2v)\n",
        "\n",
        "X_new_w2v = np.array(X_w2v)"
      ],
      "metadata": {
        "id": "sPRQrnjq8-AH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d809846-27e1-4bbe-fde5-7e0c433e57db"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ứng_dụng', 'luyện', 'đầy_đủ', 'cấp_độ']\n",
            "['quảng_cáo']\n",
            "['hiệu_quả', 'tuyệt_vời']\n",
            "['quảng_cáo', 'kinh_khủng']\n",
            "['ứng_dụng', 'đầy_đủ']\n",
            "['quảng_cáo', 'âm_thanh', 'tạp_âm']\n",
            "['quảng_cáo', 'quảng_cáo', 'chục', 'tua', 'một_chút', 'quảng_cáo', 'hát_hò', 'điên']\n",
            "['quảng_cáo', 'hoài_phiền']\n",
            "['ứng_dụng', 'hữu_ích', 'bổ_sung', 'memo', 'đoạn', 'văn']\n",
            "['hữu_ích_tra', 'nhanh_nghĩa', 'chính_xác', 'chữ', 'hán', 'tuyệt_vời']\n",
            "['rất', 'hay', 'ý_nghĩa', 'bé']\n",
            "['rất', 'không', 'quảng_cáo', 'rất', 'thoải_mái']\n",
            "['rất', 'tốt', 'bé', 'nhà', 'rất', 'thích']\n",
            "['rất', 'hay', 'bổ_ích', 'thú_vị']\n",
            "['tuyệt_vời', 'rất', 'vui_vẻ']\n",
            "['ứng_dụng', 'rất', 'hay', 'bổ_ích']\n",
            "['rất', 'hay', 'hiệu_quả', 'bé']\n",
            "['dino', 'đi', 'học', 'rất', 'tốt']\n",
            "['app', 'tệ', 'quá', 'lúc', 'không', 'gửi', 'mã_otp', 'lúc', 'kết_nối', 'thất_bại', 'lũ', 'ngu', 'làm', 'app']\n",
            "['chỗ', 'ghép', 'hàng', 'đủ', 'số', 'chữ', 'con', 'làm', 'đúng', 'rồi', 'bảo', 'sai']\n",
            "['trò', 'đã', 'bạn', 'nhiều', 'kiến_thức']\n",
            "['app', 'học', 'tốt', 'giúp', 'con', 'phát_triển', 'tư_duy']\n",
            "['ok']\n",
            "['app', 'tuyệt_vời', 'con', 'có_thể', 'vừa', 'học', 'vừa', 'chơi', 'nâng', 'cao', 'kiến_thức', 'khả_năng', 'tư_duy', 'nhạy_bén']\n",
            "['ứng_dụng', 'rất', 'tốt', 'việc', 'tư_duy', 'trí_tuệ', 'con', 'trẻ', 'bé', 'nhà', 'rất', 'hứng_thú', 'chương_trình', 'bkids']\n",
            "['chưa', 'con', 'thấy', 'app', 'rất', 'hữu_ích', 'cháu']\n",
            "['rất', 'phù_hợp', 'lứa', 'tuổi', 'cháu', 'nhà', 'người', 'nên', 'thử']\n",
            "['app', 'rất', 'hay', 'bổ_ích', 'con', 'tư_duy', 'học_hỏi', 'nhiều', 'cảm_ơn', 'bkids']\n",
            "['thi', 'online', 'kết_quả', 'đạt', 'không', 'đạt', 'không', 'công_bằng', 'làm', 'mất', 'thời_gian', 'luyện_tập', 'câu', 'hỏi', 'không', 'phù_hợp', 'chương_trình', 'học', 'đánh', 'lụi', 'hên', 'đúng', 'điểm', 'cao', 'hơn', 'điểm', 'bạn', 'khác', 'rất', 'nhiều', 'không', 'đạt', 'rà_soát', 'kiểu', 'điểm', 'thấp', 'đạt', 'điểm', 'cao', 'không']\n",
            "['thật_sự', 'không', 'phù_hợp', 'lứa', 'tuổi', 'tí', 'phần_mềm', 'ngáo_ngơ', 'dã_man', 'đứa', 'tuổi', 'người_lớn', 'làm', 'lần', 'mới', 'kịp_thời_gian', 'đồ', 'họa', 'nhận', 'chậm', 'vô_cùng']\n",
            "['không', 'học']\n",
            "['ứng_dụng', 'tuyệt_vời', 'bản_thân', 'vẫn', 'thích_cực', 'giao_diện', 'quá', 'đỉnh', 'nội_dung', 'khỏi', 'phải', 'bàn', 'quá', 'phù_hợp', 'giúp', 'bé', 'phát_triển', 'tư_duy']\n",
            "['sản_phẩm', 'rất', 'haygiúp', 'có_thể', 'chơi', 'dạy', 'con', 'mìnhrất_ý', 'nghĩamà', 'sản_phẩm', 'đội_ngũ', 'người', 'việtrất', 'bất_ngờ']\n",
            "['giao_diện', 'đẹp', 'nhiều', 'trò_chơi', 'bé', 'không', 'bị', 'chán']\n",
            "['rất', 'hài_lòng', 'ứng_dụng', 'rất', 'tốt', 'dễ', 'học']\n",
            "['tuyệt_vời']\n",
            "['cứ', 'báo', 'kết_nối', 'mạng', 'bạn', 'không', 'ổn_định', 'khi', 'mạng', 'nhà', 'rất', 'mạnh', 'cho_nên', 'bạn', 'đừng', 'tải']\n",
            "['ứng_dụng', 'rất']\n",
            "['học', 'tiếng', 'rất', 'hay']\n",
            "['trò_chơi', 'monkey', 'math', 'vui', 'tym', 'tum']\n",
            "['dữ_lăm_nha', 'người', 'tải']\n",
            "['học', 'chơi', 'chơi', 'học']\n",
            "['no', 'rất', 'hay', 'luôn']\n",
            "['kêu', 'vui_lòng', 'kiểm_tra', 'mạng', 'khi', 'mạng', 'nhà', 'rất', 'mạnh']\n",
            "['app', 'dạy', 'tiếng', 'cũng', 'bình_thường', 'giao_diện', 'đơn_giản', 'trò_chơi', 'cũng', 'đơn_giản', 'nói_chung', 'app', 'không', 'ho']\n",
            "['dùng', 'tốt', 'nghe', 'dễ', 'hiểu']\n",
            "['hơi', 'chán']\n",
            "['tốt']\n",
            "['mất', 'tiền', 'mới', 'học']\n",
            "['app', 'học', 'khá', 'ổn', 'cần', 'bổ_sung', 'thêm', 'phần', 'luyện', 'nói']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PhoBERT model"
      ],
      "metadata": {
        "id": "J1BKoPnr6cAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Load PhoBERT base model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
        "phoBert_model = AutoModel.from_pretrained(\"vinai/phobert-base\")"
      ],
      "metadata": {
        "id": "cScZLOIi6rWv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embedding(text_tokens, model):\n",
        "    \"\"\"\n",
        "    text_tokens: list of tokens (joined by whitespace), e.g., ['quảng_cáo', 'đầy_đủ']\n",
        "    \"\"\"\n",
        "    sentence = \" \".join(text_tokens)  # Join tokens into a single string\n",
        "    inputs = tokenizer(\n",
        "      sentence,\n",
        "      return_tensors=\"pt\",\n",
        "      truncation=True,\n",
        "      padding=True,\n",
        "      max_length=128  # or 256 depending on your content\n",
        "      )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the average of the last hidden state (you could also use [CLS] token)\n",
        "    last_hidden_state = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
        "    embedding = torch.mean(last_hidden_state, dim=1)  # mean pooling over tokens\n",
        "    return embedding.squeeze().numpy()"
      ],
      "metadata": {
        "id": "Sb8xGYfv6tNC"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pb = []\n",
        "\n",
        "for tokens in processed_words:\n",
        "    embedding = get_bert_embedding(tokens, phoBert_model)\n",
        "    X_pb.append(embedding)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_pb = scaler.fit_transform(X_pb)\n",
        "\n",
        "X_new_pb = np.array(X_pb)"
      ],
      "metadata": {
        "id": "baLuKUMH7FsJ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA # Not working for cases since PCA requires n_components requires the n_samples at least equal to its size (n_components ≤ min(n_samples, n_features))\n",
        "\n",
        "# apply PCA on PhoBERT vectors (e.g., from 768 → 10):\n",
        "pca = PCA(n_components=10)\n",
        "X_new_pb = pca.fit_transform(X_new_pb)\n",
        "\n",
        "# from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# svd = TruncatedSVD(n_components=100)\n",
        "# X_pb = svd.fit_transform(X_pb)"
      ],
      "metadata": {
        "id": "TjXdJXGEFMbg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump\n",
        "# Save both the scaler and PCA\n",
        "dump(scaler, 'scaler_pb.joblib')\n",
        "dump(pca, 'pca_pb.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCq__7BRl0be",
        "outputId": "703c3f03-a760-488f-c45b-6d93c724e0db"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pca_pb.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply model on data"
      ],
      "metadata": {
        "id": "GHD1dD4W64QR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of Word2Vec features:\", X_new_w2v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HH-pi3W86lC",
        "outputId": "78e392b8-f062-4bc7-f493-4dbeda4d381c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Word2Vec features: (50, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of PhoBERT features:\", X_new_pb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fDiAEExBrgD",
        "outputId": "c198bfba-0336-4437-f396-825df9d586a1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of PhoBERT features: (50, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Dependent - output\n",
        "# y = pd.get_dummies(data['sentiment']) # create 2 new cols: 0 and 1\n",
        "# y = y.iloc[:,0].values # 0 = first column; -1 = last column"
      ],
      "metadata": {
        "id": "Hb2IHCwI9eq3"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = data['sentiment'] # since the value of sentiment column is already a binary"
      ],
      "metadata": {
        "id": "bcp7R80C9gY4"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform X from array to dataframe\n",
        "df_w2v = pd.DataFrame(X_new_w2v)\n",
        "df_pb = pd.DataFrame(X_new_pb)\n",
        "\n",
        "# df.columns = [f'feature_{i}' for i in range(df.shape[1])]\n",
        "\n",
        "X_new_w2v = df_w2v\n",
        "X_new_pb = df_pb"
      ],
      "metadata": {
        "id": "56pqfzaHB-C-"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train test split"
      ],
      "metadata": {
        "id": "js9IId6AAIIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X_new_w2v, y, test_size = 0.20, random_state = 0)\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X_new_pb, y, test_size = 0.20, random_state = 0)"
      ],
      "metadata": {
        "id": "5GPajzJOC9tr"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML model"
      ],
      "metadata": {
        "id": "0Y3cOoqcAKvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier()"
      ],
      "metadata": {
        "id": "1C8sIRf9DPkj"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W2V_classifier = classifier.fit(X1_train, y1_train) # cannot handle 0, null values from train data"
      ],
      "metadata": {
        "id": "wudld57ADYGG"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PB_classifier = classifier.fit(X2_train, y2_train) # cannot handle 0, null values from train data"
      ],
      "metadata": {
        "id": "sMa-yjJ-CE0f"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict on test set"
      ],
      "metadata": {
        "id": "SlajwjwYANgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y1_pred = W2V_classifier.predict(X1_test)"
      ],
      "metadata": {
        "id": "wy_pqgAqD3ly"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y2_pred = PB_classifier.predict(X2_test)"
      ],
      "metadata": {
        "id": "omKMfuFACHfm"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation metrics"
      ],
      "metadata": {
        "id": "T8Zw-WntAVcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "iCSLEoA_D9lQ"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y1_test, y1_pred))\n",
        "print(accuracy_score(y2_test, y2_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyUjfw6uCKhv",
        "outputId": "e5aaa1b2-c348-4449-a9ae-7105fb33301b"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9\n",
            "0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y1_test, y1_pred))\n",
        "print(classification_report(y2_test, y2_pred))"
      ],
      "metadata": {
        "id": "JthmUrD_EJsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b914eaa5-562f-4ce9-80af-783173a87151"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.90        10\n",
            "   macro avg       0.45      0.50      0.47        10\n",
            "weighted avg       0.81      0.90      0.85        10\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67         1\n",
            "           1       1.00      0.89      0.94         9\n",
            "\n",
            "    accuracy                           0.90        10\n",
            "   macro avg       0.75      0.94      0.80        10\n",
            "weighted avg       0.95      0.90      0.91        10\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(W2V_classifier, X1_train, y1_train, cv=skf, scoring=\"recall\")\n",
        "print(\"W2V: precision (5‑fold CV):\", scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os53lM1Ks13i",
        "outputId": "0e6105ac-9b52-4d2e-aa22-64d196208a45"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W2V: precision (5‑fold CV): [0.33333333 1.         1.         1.         1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(PB_classifier, X2_train, y2_train, cv=skf, scoring=\"precision\")\n",
        "print(\"PB: recall (5‑fold CV):\", scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYGUrEMFs9y2",
        "outputId": "374f1a58-1d4a-4eac-9e6f-89117bb61db6"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PB: recall (5‑fold CV): [0.83333333 0.66666667 0.83333333 0.83333333 0.71428571]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print(\"W2v - Train labels:\", Counter(y1_train))\n",
        "print(\"W2v - Test  labels:\", Counter(y1_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PfClWYOrAKM",
        "outputId": "9cfaf612-2652-4bb0-91d3-6164e57d74ba"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W2v - Train labels: Counter({1: 26, 0: 14})\n",
            "W2v - Test  labels: Counter({1: 9, 0: 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y1_test, y1_pred, labels=[0,1])\n",
        "print(\"Confusion matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqHzoq9CrS44",
        "outputId": "ead87a1a-89eb-4aa2-d63e-b5a4c1f629c7"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix:\n",
            " [[0 1]\n",
            " [0 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y2_test, y2_pred, labels=[0,1])\n",
        "print(\"Confusion matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lWu5pk3rXmM",
        "outputId": "ffe61e07-b6e3-4cc7-9c12-7d635f7d2900"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix:\n",
            " [[1 0]\n",
            " [1 8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classify on new data"
      ],
      "metadata": {
        "id": "R5xEtgz5AYvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new_text = \"tạp âm ! quảng cáo hoài phiền lắm\" # PB better\n",
        "# new_text = \"Bài học này hay và thú vị! Mình học xong rồi.\" # Both\n",
        "# new_text = \"Mình ấn vào ứng dụng xong bị out ra luôn dù mk đã xoá app và tải lại rồi, vừa cập nhật phiên bản mới xong cũng k vào được luôn\" # PB better\n",
        "new_text = \"Thích phiên bản cũ có lớp cộng đồng hơn\" # None (but this case is hard)\n",
        "\n",
        "# Preprocess the new text\n",
        "processed_text = preprocess_review(new_text)\n",
        "\n",
        "## WORD2VEC\n",
        "# Get the vector representation using Word2Vec model\n",
        "text_vector = np.mean([w2v_model.wv[word] for word in processed_text if word in w2v_model.wv.index_to_key], axis=0)\n",
        "\n",
        "w2v_prediction = W2V_classifier.predict([text_vector])\n",
        "print(\"W2V Predicted class:\", w2v_prediction)\n",
        "\n",
        "## PhoBERT\n",
        "from joblib import load\n",
        "\n",
        "# Load scaler and PCA\n",
        "scaler = load('scaler_pb.joblib')\n",
        "pca = load('pca_pb.joblib')\n",
        "\n",
        "# Get PhoBERT embedding for the input sentence\n",
        "embedding = get_bert_embedding(processed_text, phoBert_model)  # assuming this returns a single 768-d vector\n",
        "X_new_pb = [embedding]  # Wrap in a list to make it 2D\n",
        "\n",
        "# Apply saved scaler and PCA\n",
        "text_vector_pb = scaler.transform(X_new_pb)\n",
        "text_vector = pca.transform(text_vector_pb)\n",
        "\n",
        "pb_prediction = PB_classifier.predict(text_vector)\n",
        "print(\"PB Predicted class:\", pb_prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jUst6HYHgms",
        "outputId": "1f1b60bd-9a81-463e-b205-7fede1fd2d6e"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W2V Predicted class: [1]\n",
            "PB Predicted class: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply hyperparameter tuning in random forest"
      ],
      "metadata": {
        "id": "DJzheWi-EY17"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uibx3NvlQZgw"
      },
      "execution_count": 105,
      "outputs": []
    }
  ]
}